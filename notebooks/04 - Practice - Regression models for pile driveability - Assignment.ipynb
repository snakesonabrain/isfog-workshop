{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice - Regression models for pile driveability - Assignment\n",
    "\n",
    "In the notebook, a Support Vector Regressor will be trained using the RBF (Radial Basis Function). This example is included to show that the mathematical details of a machine learning model are sometimes beyond the abilities of data scientist but a basic insight in the internal workings of the model can still be useful.\n",
    "\n",
    "Details on the RBF kernel can be found here (https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RBF.html) with an example of several kernel functions given here (https://scikit-learn.org/stable/auto_examples/svm/plot_svm_regression.html?highlight=svr). \n",
    "\n",
    "Support vector regression (https://scikit-learn.org/stable/modules/svm.html#svm-regression) tries to draw linear boundaries in the parameter space but the kernel functions allow non-linear boundaries to be created by transformation of the parameter space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package imports\n",
    "\n",
    "A number of Python packages are required. Numpy, Pandas and Plotly are know from the previous tutorial.  scikit-learn is a comprehensive Python package for machine learning which will be used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from plotly import subplots\n",
    "import plotly.graph_objs as go\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pile driving data\n",
    "\n",
    "### Data import\n",
    "\n",
    "The same data used before can be imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "____ = pd.___(\"Data/____.csv\")  # Store the contents of the csv file in the variable 'data'\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing operator dependence\n",
    "\n",
    "In the tutorial, it was observed that blowcounts were always around 75 blows/m. This is because the hammer operator will try to keep the blowcount steady. A better target can therefore be selected when the number of blows is multiplied by the hammer energy. If the soil generates more resistance, this target will increase, even when the blowcount is steady. The following formula can be used:\n",
    "\n",
    "$$ \\text{Normalised total ENTHRU per distance} = \\text{Blowcount} \\cdot \\text{Normalised ENTRHU} $$\n",
    "\n",
    "It is easy to implement this with Pandas. Perform the calculation and stored the result in the colum ``Normalised total ENTRHU per distance [-/m]``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['____'] = data['____'] * data['____']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shaft resistance proxy\n",
    "\n",
    "The engineered feature for approximating shaft resistance can be calculated.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_data = pd.DataFrame() # Create a dataframe for the data enhanced with the shaft friction feature\n",
    "for location in data['Location ID'].unique(): # Loop over all unique locations\n",
    "    # Select the location-specific data\n",
    "    locationdata = data[data['Location ID']==location].copy() \n",
    "    # Calculate the shaft resistance feature\n",
    "    locationdata[\"Rs [kN]\"] = \\\n",
    "        (np.pi * locationdata[\"Diameter [m]\"] * locationdata[\"z [m]\"].diff() * locationdata[\"qc [MPa]\"]).cumsum()\n",
    "    # Combine data for the different locations in 1 dataframe\n",
    "    enhanced_data = pd.concat([enhanced_data, locationdata]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation of new target variable\n",
    "\n",
    "A plot can be created to show the variation of the new target variable vs cone tip resistance, depth and $ R_s $. The plot has three panels, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = subplots.make_subplots(rows=1, cols=3, print_grid=False, shared_yaxes=True)\n",
    "trace1 = go.Scatter(\n",
    "    x=enhanced_data[\"____\"], y=enhanced_data[\"____\"], showlegend=False,\n",
    "    mode='markers',name='qc')\n",
    "fig.append_trace(trace1, 1, 1)\n",
    "trace2 = go.Scatter(\n",
    "    x=enhanced_data[\"____\"], y=enhanced_data[\"____\"], showlegend=False,\n",
    "    mode='markers',name='z')\n",
    "fig.append_trace(trace2, 1, 2)\n",
    "trace3 = go.Scatter(\n",
    "    x=enhanced_data[\"____\"], y=enhanced_data[\"____\"], showlegend=False,\n",
    "    mode='markers',name='Rs')\n",
    "fig.append_trace(trace3, 1, 3)\n",
    "\n",
    "fig['layout']['xaxis1'].update(title='qc [MPa]', range=(0, 100), dtick=10)\n",
    "fig['layout']['xaxis2'].update(title='z [m]', range=(0, 40), dtick=5)\n",
    "fig['layout']['xaxis3'].update(title='Rs [kN]', range=(0, 12e3))\n",
    "fig['layout']['yaxis1'].update(title='Normalised total ENTHRU [-/m]')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data for machine learning\n",
    "\n",
    "NaN values can be dropped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-splitting\n",
    "\n",
    "A location-based train-test split is used. We will create a deep copy of the resulting dataframe. Otherwise, Pandas will update the original dataframe when we make changes, which is undesirable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_ids = ['EL', 'CB', 'AV', 'BV', 'EF', 'DL', 'BM']\n",
    "# Training data - ID not in validation_ids\n",
    "training_data = deepcopy(enhanced_data[~enhanced_data['Location ID'].isin(validation_ids)])\n",
    "# Validation data - ID in validation_ids\n",
    "validation_data = deepcopy(enhanced_data[enhanced_data['Location ID'].isin(validation_ids)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building\n",
    "\n",
    "### Support vector regression import\n",
    "\n",
    "The SVR class can be imported (see scikit-learn docs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import ____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR instance\n",
    "\n",
    "An SVR instance can be created using the ``rbf`` kernel function and with other hyperparameters kept at their defaults. Can you identify from the documentation how these hyperparameters are determined and what their meaning is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_rbf = SVR(kernel=\"____\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "\n",
    "The features on which we will train can be selected. Initially, we can work with a single feature ``Rs [kN]``. Can you add features to improve the accuracy of the model?\n",
    "\n",
    "The target is our new normalised total ENTHRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"____\"]\n",
    "X = training_data[features]\n",
    "y = training_data[\"____\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "\n",
    "We can train the model using ``Rs [kN]`` as the single feature and the total normalised ENTHRU as the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_rbf.____(____, ____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model predictions\n",
    "\n",
    "### Training set\n",
    "\n",
    "Making predictions is as easy as running the ``predict`` method on the trained model. We can assign the result to ``y_pred_train``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "____ = svr_rbf.____(____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions can be visualised against the observed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = subplots.make_subplots(rows=1, cols=1, print_grid=False, shared_yaxes=True)\n",
    "trace1 = go.Scatter(\n",
    "    x=enhanced_data[\"Rs [kN]\"], y=enhanced_data[\"Normalised total ENTRHU per distance [-/m]\"], showlegend=True,\n",
    "    mode='markers',name='True values')\n",
    "fig.append_trace(trace1, 1, 1)\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x=____[\"Rs [kN]\"], y=____, showlegend=True,\n",
    "    mode='markers',name='Predictions')\n",
    "fig.append_trace(trace1, 1, 1)\n",
    "\n",
    "fig['layout']['xaxis1'].update(title='Rs [kN]', range=(0, 12e3))\n",
    "fig['layout']['yaxis1'].update(title='Normalised total ENTHRU [-/m]')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $ R^2 $-score can be determined using the ``score`` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_rbf.____(____, ____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set\n",
    "\n",
    "The features and target can be selected for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = validation_data[____]\n",
    "y_test = validation_data[\"____\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions can again be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = svr_rbf.____(____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the $ R^2 $-score can be displayed. Does the model generalise well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_rbf.____(____, ____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted that the predictions are normalised total ENTRHU per unit length. To predict blowcount, we need to divide by the normalised ENTHRU. We can first assign the predictions to the column ``'Predictions'`` in the ``validation_data`` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "____['____'] = y_pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted blowcount can then be calculated in a single line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data['Predicted blowcount'] = validation_data['____'] / validation_data['____']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for selected locations\n",
    "\n",
    "The results for selected locations in the test set can be visualised by overlaying predictions and observed values for the total normalised ENTRHU per unit length and blowcount.\n",
    "\n",
    "The results show that the model generally performs well. However, the increasing detrimental effect of blowcount underprediction towards the final pile penetration is not captures in any of the scikit-learn metrics. Custom coding is required to get more insight in this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available locations are 'EL', 'CB', 'AV', 'BV', 'EF', 'DL', 'BM'\n",
    "selected_location = 'EL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = subplots.make_subplots(rows=1, cols=2, print_grid=False, shared_yaxes=True)\n",
    "\n",
    "loc_data = validation_data[validation_data['Location ID'] == ____]\n",
    "trace_observations = go.Scatter(\n",
    "    x=loc_data['Normalised total ENTRHU per distance [-/m]'],\n",
    "    y=loc_data['____'], showlegend=True, mode='lines',name='Observed values')\n",
    "fig.append_trace(trace_observations, 1, 1)\n",
    "\n",
    "trace_predictions = go.Scatter(\n",
    "    x=loc_data['Predictions'],\n",
    "    y=loc_data['z [m]'], showlegend=True, mode='markers',name='Predictions')\n",
    "fig.append_trace(trace_predictions, 1, 1)\n",
    "\n",
    "trace_observations_blct = go.Scatter(\n",
    "    x=loc_data['____'],\n",
    "    y=loc_data['z [m]'], showlegend=True, mode='lines',name='Observed blowcount')\n",
    "fig.append_trace(trace_observations_blct, 1, 2)\n",
    "\n",
    "trace_predictions_blct = go.Scatter(\n",
    "    x=loc_data['____'],\n",
    "    y=loc_data['z [m]'], showlegend=True, mode='markers',name='Predicted blowcount')\n",
    "fig.append_trace(trace_predictions_blct, 1, 2)\n",
    "\n",
    "fig['layout']['xaxis1'].update(title='Normalised total ENTRHU [-/m]', side='top', anchor='y')\n",
    "fig['layout']['xaxis2'].update(title='Blowcount [Blows/m]', side='top', anchor='y')\n",
    "fig['layout']['yaxis1'].update(title='Depth below mudline [m]', autorange='reversed')\n",
    "fig['layout'].update(height=700, width=900)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifications\n",
    "\n",
    "When the initial model performs as expected, you can investigate the effect of adding more features and changing the hyperparameters. Can you get even better scores by doing this.\n",
    "\n",
    "In scikit-learn, there are methods such as ``GridSearchCV`` which allow selection of an optimal set of hyperparameters but discussing these methods is beyond the scope of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
