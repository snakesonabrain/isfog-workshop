{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of building regression models\n",
    "\n",
    "Regression models are used to define a model which predicts a continuous variable from a number of observations. Regression modelling is very common in (geotechnical) engineering and is by no means a new thing. Most of the correlations used to infer geotechnical parameters from measurements are indeed regression models.\n",
    "\n",
    "In machine learning, regression modelling is a common task and several model types exist to define relations between the observations (features) and the outcome (target).\n",
    "\n",
    "In this notebook, we will show how a simple regression model is created, how it is trained and how the model accuracy is assessed. The demo from the ISFOG pile driving prediction event is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package imports\n",
    "\n",
    "A number of Python packages are required. Numpy, Pandas and Plotly are know from the previous tutorial.  scikit-learn is a comprehensive Python package for machine learning which will be used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from plotly import subplots\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pile driving data\n",
    "\n",
    "The dataset is kindly provided by [Cathie Group](http://www.cathiegroup.com).\n",
    "\n",
    "### Importing data\n",
    "\n",
    "The first step in any data science exercise is to get familiar with the data. The data is provided in a csv file (```training_data.csv```). We can import the data with Pandas and display the first five rows using the ```head()``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Data/training_data_piles.csv\")  # Store the contents of the csv file in the variable 'data'\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has 12 columns, containing PCPT data ($ q_c $, $ f_s $ and $ u_2 $), recorded hammer data (blowcount, normalised hammer energy, normalised ENTHRU and total number of blows), pile data (diameter, bottom wall thickness and pile final penetration). A unique ID identifies the location and $ z $ defines the depth below the mudline.\n",
    "\n",
    "The data has already been resampled to a regular grid with 0.5m grid intervals to facilitate the further data handling.\n",
    "\n",
    "The hammer energy has been normalised using the same reference energy for all piles in this prediction exercise.\n",
    "\n",
    "We can see that there is no driving data in the first five rows (NaN values), this is because driving only started after a given self-weight penetration of the pile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary statistics\n",
    "\n",
    "We can easily create summary statistics of each column using the ```describe()``` function on the data. This gives us the number of elements, mean, standard deviation, minimum, maximum and percentiles of each column of the data.\n",
    "\n",
    "We can see that there are more PCPT data points than hammer data points. This makes sense as there is soil data available above the pile self-weight penetration and below the final pile penetration. The pile data is defined in the self-weight penetration part of the profile, so there are slightly more pile data points than hammer record data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting\n",
    "\n",
    "We can plot the cone tip resistance, blowcount and normalised ENTHRU energy for all locations to show how the data varies with depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = subplots.make_subplots(rows=1, cols=3, print_grid=False, shared_yaxes=True)\n",
    "trace1 = go.Scatter(x=data[\"qc [MPa]\"], y=data[\"z [m]\"], showlegend=False, mode='markers',name='qc')\n",
    "fig.append_trace(trace1, 1, 1)\n",
    "trace2 = go.Scatter(x=data[\"Normalised ENTRHU [-]\"], y=data[\"z [m]\"], showlegend=False,\n",
    "                    mode='markers',name='ENTRHU')\n",
    "fig.append_trace(trace2, 1, 2)\n",
    "trace3 = go.Scatter(x=data[\"Blowcount [Blows/m]\"], y=data[\"z [m]\"], showlegend=False,\n",
    "                    mode='markers',name='Blowcount')\n",
    "fig.append_trace(trace3, 1, 3)\n",
    "fig['layout']['xaxis1'].update(title=r'$ q_c \\ \\text{[MPa]} $', side='top', anchor='y', range=(0, 100), dtick=10)\n",
    "fig['layout']['xaxis2'].update(title=r'$ \\text{Normalised ENTRHU [-]} $', side='top', anchor='y',\n",
    "                               range=(0, 1), dtick=0.2)\n",
    "fig['layout']['xaxis3'].update(title=r'$ \\text{Observed blowcount [Blows/m]} $', side='top', anchor='y',\n",
    "                               range=(0, 200), dtick=25)\n",
    "fig['layout']['yaxis1'].update(title=r'$ z \\ \\text{[m]} $', range=(40, 0), dtick=2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cone resistance data shows that the site mainly consists of sand of varying relative density. In certain profiles, clay is present below 10m. There are also locations with very high cone resistance (>70MPa).\n",
    "\n",
    "The blowcount profile shows that blowcount is relatively well clustered around a generally increasing trend with depth. The normalised ENTHRU energy is also increasing with depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can isolate the data for a single location by selecting this data from the dataframe with all data. As an example, we can do this for location <i>EK</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the data where the column 'Location ID' is equal to the location name\n",
    "location_data = data[data[\"Location ID\"] == \"EK\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the data for this location on top of the general data cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace1_EK = go.Scatter(\n",
    "    x=location_data[\"qc [MPa]\"], y=location_data[\"z [m]\"], showlegend=False, mode='lines',name='qc EK')\n",
    "fig.append_trace(trace1_EK, 1, 1)\n",
    "trace2_EK = go.Scatter(\n",
    "    x=location_data[\"Normalised ENTRHU [-]\"], y=location_data[\"z [m]\"], showlegend=False,\n",
    "    mode='lines',name='ENTRHU EK')\n",
    "fig.append_trace(trace2_EK, 1, 2)\n",
    "trace3_EK = go.Scatter(\n",
    "    x=location_data[\"Blowcount [Blows/m]\"], y=location_data[\"z [m]\"], showlegend=False,\n",
    "    mode='lines',name='Blowcount EK')\n",
    "fig.append_trace(trace3_EK, 1, 3)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that pile driving started from 5m depth and continued until a depth of 30m, when the pile tip reached a sand layer with $ q_c $ > 60MPa.\n",
    "\n",
    "Feel free to investigate the soil profile and driving data for the other locations by changing the location ID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of the prediction event, we are interested in the variation of blowcount with $ q_c $, hammer energy, ... We can also generate plots to see the correlations. The data shows significant scatter and non-linear behaviour. We will take this into account for our machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = subplots.make_subplots(rows=1, cols=3, print_grid=False, shared_yaxes=True)\n",
    "trace1 = go.Scatter(\n",
    "    x=data[\"qc [MPa]\"], y=data[\"Blowcount [Blows/m]\"], showlegend=False, mode='markers',name='qc')\n",
    "fig.append_trace(trace1, 1, 1)\n",
    "trace2 = go.Scatter(x=data[\"Normalised ENTRHU [-]\"], y=data[\"Blowcount [Blows/m]\"], showlegend=False,\n",
    "                    mode='markers',name='ENTRHU')\n",
    "fig.append_trace(trace2, 1, 2)\n",
    "trace3 = go.Scatter(x=data[\"z [m]\"], y=data[\"Blowcount [Blows/m]\"], showlegend=False,\n",
    "                    mode='markers',name='z')\n",
    "fig.append_trace(trace3, 1, 3)\n",
    "fig['layout']['xaxis1'].update(title=r'$ q_c \\ \\text{[MPa]} $', range=(0, 100), dtick=10)\n",
    "fig['layout']['xaxis2'].update(title=r'$ \\text{Normalised ENTRHU [-]} $', range=(0, 1), dtick=0.2)\n",
    "fig['layout']['xaxis3'].update(title=r'$ z \\ \\text{[m]} $', range=(0, 40), dtick=5)\n",
    "fig['layout']['yaxis1'].update(title=r'$ \\text{Observed blowcount [Blows/m]} $', range=(0, 200), dtick=25)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of machine learning\n",
    "\n",
    "The goal of the prediction exercise is to define a model relating the input (soil data, hammer energy, pile data) with the output (blowcount).\n",
    "\n",
    "In ML terminology, we call the inputs (the columns of the dataset except for the blowcount) <i>features</i>. The blowcount is the <i>target variable</i>. Each row in the dataframe represents a <i>sample</i>, a combination of feature values for which the output is known. Data for which a value of the target variable is not yet available is called <i>unseen data</i>.\n",
    "\n",
    "Before we dive into the code for generating ML models, let's discuss some of the concepts in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning techniques\n",
    "\n",
    "ML combines several data science techniques under one general denominator. We can discern the following families:\n",
    "\n",
    "   - Classification: Predict the value of a discrete target variable of a data point based on its features\n",
    "   - Regression: Predict the value of a continuous target variable based on its features\n",
    "   - Clustering: Identify groups of similar data points based on their features\n",
    "   - Dimensionality reduction: Identify the features with the greatest influence on the data\n",
    "   \n",
    "The first techniques are examples of <i>supervised learning</i>. We will use data where the output has been observed and use that to <i>train</i> the ML model. Training a model is essentially the optimisation of the coefficients of a mathematical model to minimise the difference between model predictions and observed values. Such a trained algorithm is then capable of making predictions for unseen data.\n",
    "\n",
    "<img src=\"Images/machine_learning_concept.png\">\n",
    "<br><center><b>Sketch of the machine learning concept</b></center>\n",
    "\n",
    "This concept is not fundamentally different from any other type of data-driven modelling. The main advantage of the ML approach is the speed at which the models can be trained and the many types of models available to the engineer.\n",
    "\n",
    "In our example of pile driving, we have a <b>regression</b> problem where we are training a model to relate features (soil data, hammer energy and pile data) with a continuous target variable (blowcount)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fitting\n",
    "\n",
    "Machine learning has disadvantages which can lead to problematic situations if the techniques are misused. One of these disadvantages is that the ML algorithm will always find a fit, even if it is a poor one.\n",
    "\n",
    "The figure below shows an example with data showing a non-linear trend between input and output with some scatter around a trend. We can identify the following situations:\n",
    "\n",
    "   - Underfitting: If we use a linear model for this data, we are not capturing the trend. The model predictions will be poor;\n",
    "   - Good fit: If we formulate a model (quadratic in this case) which captures the general trend but allows variations around the trend, we obtain a good fit. In geotechnical problems, we will never get a perfect a fit but if we identify the influence of the input parameters in a consistent manner, we can build good-quality models;\n",
    "   - Overfitting: If we have a model which perfectly fits all known data points, the prediction for an unseen data point will be poor. The influence of each measurement on the model is too important. The model overfits the data and does not capture the general trends. It just represents the data on which it was trained.\n",
    "\n",
    "<img src=\"Images/over_underfitting.png\">\n",
    "<br><center><b>Underfitting and overfitting</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model metrics\n",
    "\n",
    "To prevent misuse of ML models, we will look at certain model metrics to check the quality. There are several model metrics. Two of the more common ones are the <b>Mean Squared Error (MSE)</b> and the <b>coefficient of determination ($ R^2 $)</b>.\n",
    "\n",
    "The MSE-value is the normalised sum of quadratic differences. The closer it is to 0, the better the fit.\n",
    "\n",
    "$$ \\text{MSE}(y, \\hat{y}) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples} - 1} (y_i - \\hat{y}_i)^2. $$\n",
    "\n",
    "$ \\hat{y}_i $ is the predicted value of the i-th sample and $ y_i $ is the true (measured) value.\n",
    "\n",
    "The coefficient of determination ($ R^2 $) is a measure for a measure of how well future samples are likely to be predicted by the model. A good model has an $ R^2 $-value which is close to 1.\n",
    "\n",
    "$$ R^2(y, \\hat{y}) = 1 - \\frac{\\sum_{i=0}^{n_{\\text{samples}} - 1} (y_i - \\hat{y}_i)^2}{\\sum_{i=0}^{n_\\text{samples} - 1} (y_i - \\bar{y})^2} \\quad \\text{where} \\ \\bar{y} =  \\frac{1}{n_{\\text{samples}}} \\sum_{i=0}^{n_{\\text{samples}} - 1} y_i$$\n",
    "\n",
    "In the example, we will see how we can easily calculate these metrics from the data using the functions available in the ML Python package ```scikit-learn```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model validation\n",
    "\n",
    "When building a ML model, we will only use a subset of the data for training the model. The other subset is deliberately excluded from the learning process and used to <i>validate</i> the model. The trained model is applied on the unseen data of the validation dataset and the accuracy of the predictions is checked, resulting in a validation score representing the accuracy of the model for the validation dataset.\n",
    "\n",
    "If our trained model is of good quality, the predictions for the validation dataset will be close to the measured values.\n",
    "\n",
    "We will partition our data in a training dataset and a validation dataset. For the validation data set, we use seven piles. The other piles will be used as the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_ids = ['EL', 'CB', 'AV', 'BV', 'EF', 'DL', 'BM']\n",
    "# Training data - ID not in validation_ids\n",
    "training_data = data[~data['Location ID'].isin(validation_ids)]\n",
    "# Validation data - ID in validation_ids\n",
    "validation_data = data[data['Location ID'].isin(validation_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these concepts in mind, we can start building up a simple ML model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic machine learning example: Linear modelling\n",
    "\n",
    "The most basic type of ML model is a linear model. We are already using linear models in a variety of applications and often fit them without making use of ML techniques. The general equation for a linear model is given below for a model with $ N $ features:\n",
    "\n",
    "$$ y = a_0 + a_1 \\cdot x_1 + a_2 \\cdot x_2 + ... + a_N \\cdot x_N + \\epsilon $$\n",
    "\n",
    "where $ \\epsilon $ is the estimation error.\n",
    "\n",
    "Based on the training dataset, the value of the coefficients ($ a_0, a_1, ..., a_N $) is determined using optimisation techniques to minimise the difference between measured and predicted values. As the equation shows, a good fit will be obtained when the relation between output and inputs is truly linear. If there are non-linearities in the data, the fit will be less good.\n",
    "\n",
    "We will illustrate how a linear regression machine learning model is generated from the available driving data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model based on normalised ENTHRU only\n",
    "\n",
    "The simplest linear model depends on only one feature. We can select the normalised energy transmitted to the pile (ENTRHU) as the only feature for illustration purposes.\n",
    "\n",
    "The mathematical form of the model can be written as:\n",
    "\n",
    "$$ BLCT = a_o + a_1 \\cdot \\text{ENTRHU}_{norm} + \\epsilon $$\n",
    "\n",
    "We will create a dataframe $ X $ with only the normalised ENTHRU feature data and we will put the observed values of the target variable (blowcount) in the vector $ y $.\n",
    "\n",
    "Note that machine learning algorithms will raise errors when NaN values are provided. We need to ensure that we remove such values. We can creata a dataframe ```cleaned_training_data``` which only contains rows with no NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Normalised ENTRHU [-]']\n",
    "cleaned_training_data = training_data.dropna() # Remove NaN values\n",
    "X = cleaned_training_data[features]\n",
    "y = cleaned_training_data[\"Blowcount [Blows/m]\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a linear model. We need to import this type of model from the scikit-learn package. We can fit the linear model to the data using the ```fit()``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model_1 = LinearRegression().fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, our model has been trained with the data and the coefficients are known. $ a_0 $ is called the intercept and $ a_1 $ to $ a_n $ are stored in ```coef_```. Because we only have one feature, ```coef_``` only returns a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.coef_, model_1.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the data with our trained fit. We can see that the fit follows a general trend but the quality is not great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = subplots.make_subplots(rows=1, cols=1, print_grid=False) \n",
    "\n",
    "observations_data = go.Scatter(       \n",
    "    x=X['Normalised ENTRHU [-]'],\n",
    "    y=y,\n",
    "    showlegend=True,\n",
    "    mode='markers',\n",
    "    name='Data')\n",
    "fig.append_trace(observations_data, 1, 1)\n",
    "\n",
    "model_data = go.Scatter(       \n",
    "    x=np.linspace(0.0, 1, 50),\n",
    "    y=model_1.intercept_ + model_1.coef_ * np.linspace(0.0, 1, 50),\n",
    "    showlegend=True,\n",
    "    mode='lines',\n",
    "    name='model')\n",
    "fig.append_trace(model_data, 1, 1)\n",
    "\n",
    "fig['layout']['xaxis1'].update(title='Normalised ENTRHU [-]') \n",
    "fig['layout']['yaxis1'].update(title='Blowcount [Blows/m]')\n",
    "fig['layout'].update(title='Single feature model')\n",
    "fig.show()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate the $ R^2 $ score for our training data. The score is below 0.5 and it goes without saying that this model needs improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sections, we will explore ways to improve our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linearizing features\n",
    "\n",
    "When using ENTRHU as our model feature, we can see that a linear model is not the most appropriate choice as the relation between blowcount and ENTRHU is clearly non-linear. However, we can <i>linearize</i> features.\n",
    "\n",
    "For example, we can propose a relation using using a tangent hyperbolic law, which seems to fit better with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 100)\n",
    "\n",
    "fig = subplots.make_subplots(rows=1, cols=1, print_grid=False) \n",
    "\n",
    "observations_data = go.Scatter(       \n",
    "    x=X['Normalised ENTRHU [-]'],\n",
    "    y=y,\n",
    "    showlegend=True,\n",
    "    mode='markers',\n",
    "    name='Data')\n",
    "fig.append_trace(observations_data, 1, 1)\n",
    "\n",
    "tanh_data = go.Scatter(       \n",
    "    x=x,\n",
    "    y=80 * np.tanh(5 * x - 0.5),\n",
    "    showlegend=True,\n",
    "    mode='lines',\n",
    "    name='Tanh relation')\n",
    "fig.append_trace(tanh_data, 1, 1)\n",
    "\n",
    "fig['layout']['xaxis1'].update(title='Normalised ENTRHU [-]') \n",
    "fig['layout']['yaxis1'].update(title='Blowcount [Blows/m]', range=(0, 175))\n",
    "fig['layout'].update(title='Tanh relation')\n",
    "fig.show()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a linearized feature:\n",
    "\n",
    "$$ (\\text{ENTHRU})_{lin} = \\tanh(5 \\cdot \\text{ENTHRU}_{norm} - 0.5) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xlin = np.tanh(5 * cleaned_training_data[[\"Normalised ENTRHU [-]\"]] - 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xlin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When plotting the linearized data against the blowcount, we can see that a linear relation is more appropriate, although there is still a lot of scatter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 100)\n",
    "\n",
    "fig = subplots.make_subplots(rows=1, cols=1, print_grid=False) \n",
    "\n",
    "observations_data = go.Scatter(       \n",
    "    x=Xlin['Normalised ENTRHU [-]'],\n",
    "    y=y,\n",
    "    showlegend=True,\n",
    "    mode='markers',\n",
    "    name='Data')\n",
    "fig.append_trace(observations_data, 1, 1)\n",
    "\n",
    "fig['layout']['xaxis1'].update(title='Linearized normalised ENTHRU') \n",
    "fig['layout']['yaxis1'].update(title='Blowcount [Blows/m]', range=(0, 175))\n",
    "fig['layout'].update(title='Tanh relation')\n",
    "fig.show()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fit another linear model using this linearized feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = LinearRegression().fit(Xlin, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the intercept and the model coefficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.coef_, model_2.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with the linearized feature can then be written as:\n",
    "\n",
    "$$ BLCT = a_0 + a_1 \\cdot (\\text{ENTHRU})_{lin} $$ \n",
    "\n",
    "We can visualize the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = subplots.make_subplots(rows=1, cols=1, print_grid=False) \n",
    "\n",
    "observations_data = go.Scatter(       \n",
    "    x=X['Normalised ENTRHU [-]'],\n",
    "    y=y,\n",
    "    showlegend=True,\n",
    "    mode='markers',\n",
    "    name='Data')\n",
    "fig.append_trace(observations_data, 1, 1)\n",
    "\n",
    "model_data = go.Scatter(       \n",
    "    x=x,\n",
    "    y=model_2.intercept_ + model_2.coef_ * (np.tanh(5 * x - 0.5)),\n",
    "    showlegend=True,\n",
    "    mode='lines',\n",
    "    name='model')\n",
    "fig.append_trace(model_data, 1, 1)\n",
    "\n",
    "fig['layout']['xaxis1'].update(title='Normalised ENTRHU [-]') \n",
    "fig['layout']['yaxis1'].update(title='Blowcount [Blows/m]', range=(0, 175))\n",
    "fig['layout'].update(title='Improved single feature model')\n",
    "fig.show()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the $ R^2 $ model score. By linearizing the normalised ENTHRU energy, we have improved our $ R^2 $ score and are thus fitting a model which better describes our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.score(Xlin, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using engineering knowledge\n",
    "\n",
    "We know from engineering considerations on the pile driving problem that the soil resistance to driving (SRD) can be expressed as the sum of shaft friction and end bearing resistance. The shaft friction can be expressed as the integral of the unit shaft friction over the pile circumference and length.\n",
    "\n",
    "If we make the simplifying assumption that there is a proportionality between the cone resistance and the unit shaft friction ($ f_s = \\alpha \\cdot q_c $), we can write the shaft resistance as follows:\n",
    "\n",
    "$$ R_s = \\int_{0}^{L} \\alpha \\cdot q_c \\cdot \\pi \\cdot D \\cdot dz \\approx \\alpha \\cdot \\pi \\cdot D \\cdot \\sum q_{c,i} \\cdot \\Delta z $$\n",
    "\n",
    "We can create an additional feature for this. Creating features based on our engineering knowledge will often help us to introduce experience in a machine learning algorithm.\n",
    "\n",
    "To achieve this, we will create a new dataframe using our training data. We will iteration over all locations in the training data and calculate the $ R_s $ feature using a cumulative sum function. We will then put this data together for all locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_data = pd.DataFrame() # Create a dataframe for the data enhanced with the shaft friction feature\n",
    "for location in training_data['Location ID'].unique(): # Loop over all unique locations\n",
    "    # Select the location-specific data\n",
    "    locationdata = training_data[training_data['Location ID']==location].copy() \n",
    "    # Calculate the shaft resistance feature\n",
    "    locationdata[\"Rs [kN]\"] = \\\n",
    "        (np.pi * locationdata[\"Diameter [m]\"] * locationdata[\"z [m]\"].diff() * locationdata[\"qc [MPa]\"]).cumsum()\n",
    "    # Combine data for the different locations in 1 dataframe\n",
    "    enhanced_data = pd.concat([enhanced_data, locationdata]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the data to see that the clustering of our SRD shaft resistance feature vs blowcount is much better than the clustering of $ q_c $ vs blowcount. We can also linearize the relation between shaft resistance and blowcount.\n",
    "\n",
    "We can propose the following relation:\n",
    "\n",
    "$$ BLCT = 85 \\cdot \\tanh \\left( \\frac{R_s}{1000} - 1 \\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = subplots.make_subplots(rows=1, cols=2, print_grid=False, shared_yaxes=True) \n",
    "\n",
    "qc_data = go.Scatter(       \n",
    "    x=enhanced_data['qc [MPa]'],\n",
    "    y=enhanced_data['Blowcount [Blows/m]'],\n",
    "    showlegend=False,\n",
    "    mode='markers',\n",
    "    name='qc data')\n",
    "fig.append_trace(qc_data, 1, 1)\n",
    "\n",
    "shaftresistance_data = go.Scatter(       \n",
    "    x=enhanced_data['Rs [kN]'],\n",
    "    y=enhanced_data['Blowcount [Blows/m]'],\n",
    "    showlegend=False,\n",
    "    mode='markers',\n",
    "    name='Rs data')\n",
    "fig.append_trace(shaftresistance_data, 1, 2)\n",
    "\n",
    "x = np.linspace(0.0, 12000, 50)\n",
    "\n",
    "model_data = go.Scatter(       \n",
    "    x=x,\n",
    "    y=85 * (np.tanh(0.001*x-1)),\n",
    "    showlegend=False,\n",
    "    mode='lines',\n",
    "    name='model')\n",
    "fig.append_trace(model_data, 1, 2)\n",
    "\n",
    "fig['layout']['xaxis1'].update(title='qc [MPa]') \n",
    "fig['layout']['xaxis2'].update(title='Rs [kN]') \n",
    "fig['layout']['yaxis1'].update(title='Blowcount [Blows/m]', range=(0, 175))\n",
    "fig['layout'].update(title='Improved single feature model')\n",
    "fig.show()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then proceed to filter the NaN values from the data and fit a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"Rs [kN]\"]\n",
    "X = enhanced_data.dropna()[features]\n",
    "y = enhanced_data.dropna()[\"Blowcount [Blows/m]\"]\n",
    "Xlin = np.tanh((0.001 * X) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = LinearRegression().fit(Xlin, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the coefficients of the linear model and visualise the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.intercept_, model_3.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = subplots.make_subplots(rows=1, cols=1, print_grid=False) \n",
    "\n",
    "observations_data = go.Scatter(       \n",
    "    x=X['Rs [kN]'],\n",
    "    y=y,\n",
    "    showlegend=True,\n",
    "    mode='markers',\n",
    "    name='Data')\n",
    "fig.append_trace(observations_data, 1, 1)\n",
    "\n",
    "x = np.linspace(0.0, 12000, 50)\n",
    "\n",
    "model_data = go.Scatter(       \n",
    "    x=x,\n",
    "    y=model_3.intercept_ + model_3.coef_ * (np.tanh(0.001 * x - 1)),\n",
    "    showlegend=True,\n",
    "    mode='lines',\n",
    "    name='model')\n",
    "fig.append_trace(model_data, 1, 1)\n",
    "\n",
    "fig['layout']['xaxis1'].update(title='Rs [kN]') \n",
    "fig['layout']['yaxis1'].update(title='Blowcount [Blows/m]', range=(0, 175))\n",
    "fig['layout'].update(title='Engineered feature model')\n",
    "fig.show()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit looks reasonable and this is also reflected in the $ R^2 $ score which is just greater than 0.6. We have shown that using engineering knowledge can greatly improve model quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.score(Xlin, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using multiple features\n",
    "\n",
    "The power of machine learning algorithms is that you can experiment with adding multiple features. Adding a feature can improve you model if it has a meaningful relation with the output.\n",
    "\n",
    "We can use our linearized relation with normalised ENTHRU, shaft resistance and we can also linearize the variation of blowcount with depth:\n",
    "\n",
    "$$ BLCT = 100 \\cdot \\tanh \\left( \\frac{z}{10} - 0.5 \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model with the combined features will take the following mathematical form:\n",
    "\n",
    "$$ BLCT = a_0 + a_1 \\cdot \\tanh \\left( 5 \\cdot \\text{ENTHRU}_{norm} - 0.5 \\right) + a_2 \\cdot \\tanh \\left( \\frac{R_s}{1000} - 1 \\right) + a_3 \\cdot \\tanh \\left( \\frac{z}{10} - 0.5 \\right) $$\n",
    "\n",
    "We can create the necessary features in our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_data[\"linearized ENTHRU\"] = np.tanh(5 * enhanced_data[\"Normalised ENTRHU [-]\"] - 0.5)\n",
    "enhanced_data[\"linearized Rs\"] = np.tanh(0.001 * enhanced_data[\"Rs [kN]\"] - 1)\n",
    "enhanced_data[\"linearized z\"] = np.tanh(0.1 * enhanced_data[\"z [m]\"] - 0.5)\n",
    "linearized_features = [\"linearized ENTHRU\", \"linearized Rs\", \"linearized z\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now fit a linear model with three features. The matrix $ X $ is now an $ n \\times 3 $ matrix ($ n $ samples and 3 features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = enhanced_data.dropna()[linearized_features]\n",
    "y = enhanced_data.dropna()[\"Blowcount [Blows/m]\"]\n",
    "model_4 = LinearRegression().fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the $ R^2 $ score. The score is slightly better compared to our previous model. Given the scatter in the data, this score is already a reasonable value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model predictions\n",
    "\n",
    "The linear regression model always allows us to write down the mathematical form of the model. We can do so here by filling in the intercept ($ a_0 $) a coefficients $ a_1 $, $ a_2 $ and $ a_3 $ in the equation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4.intercept_, model_4.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we don't need to explicitly write down the mathematical shape of the model to use it in the code. We can make predictions using the fitted model straightaway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_4.predict(X)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot these predictions together with the data. We can see that the model follows the general trend of the data fairly well. There is still significant scatter around the trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = subplots.make_subplots(rows=1, cols=3, print_grid=False, shared_yaxes=True)\n",
    "# Observations\n",
    "trace1 = go.Scatter(\n",
    "    x=enhanced_data[\"Rs [kN]\"], y=enhanced_data[\"Blowcount [Blows/m]\"],\n",
    "    showlegend=True, mode='markers',name='Rs observed')\n",
    "fig.append_trace(trace1, 1, 1)\n",
    "trace2 = go.Scatter(\n",
    "    x=enhanced_data[\"Normalised ENTRHU [-]\"], y=enhanced_data[\"Blowcount [Blows/m]\"], showlegend=True,\n",
    "                    mode='markers',name='ENTRHU observed')\n",
    "fig.append_trace(trace2, 1, 2)\n",
    "trace3 = go.Scatter(x=enhanced_data[\"z [m]\"], y=enhanced_data[\"Blowcount [Blows/m]\"], showlegend=True,\n",
    "                    mode='markers',name='z observed')\n",
    "fig.append_trace(trace3, 1, 3)\n",
    "\n",
    "# Predictions\n",
    "trace1_pred = go.Scatter(\n",
    "    x=enhanced_data.dropna()[\"Rs [kN]\"], y=predictions,\n",
    "    showlegend=True, mode='markers',name='Rs predicted')\n",
    "fig.append_trace(trace1_pred, 1, 1)\n",
    "trace2_pred = go.Scatter(\n",
    "    x=enhanced_data.dropna()[\"Normalised ENTRHU [-]\"], y=predictions, showlegend=True,\n",
    "                    mode='markers',name='ENTRHU predicted')\n",
    "fig.append_trace(trace2_pred, 1, 2)\n",
    "trace3_pred = go.Scatter(\n",
    "    x=enhanced_data.dropna()[\"z [m]\"], y=predictions, showlegend=True,\n",
    "                    mode='markers',name='z predicted')\n",
    "fig.append_trace(trace3_pred, 1, 3)\n",
    "\n",
    "fig['layout']['xaxis1'].update(title='Rs [kN]', range=(0, 12e3))\n",
    "fig['layout']['xaxis2'].update(title='Normalised ENTRHU [-]', range=(0, 1), dtick=0.2)\n",
    "fig['layout']['xaxis3'].update(title='z [m]', range=(0, 40), dtick=5)\n",
    "fig['layout']['yaxis1'].update(title='Observed blowcount [Blows/m]', range=(0, 200), dtick=25)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the prediction event, the goal is to fit a machine learning model which further refines the model developed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model validation\n",
    "\n",
    "At the start of the exercise, we excluded a couple of locations from the fitting to check how well the model would perform for these unseen locations.\n",
    "\n",
    "We can now perform this validation exercise by calculating the shaft resistance and linearizing the model features. We can then make predictions with our model developed above.\n",
    "\n",
    "We will illustrate this for location CB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the dataframe with location-specific data\n",
    "validation_data_CB = validation_data[validation_data[\"Location ID\"] == \"CB\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the shaft resistance feature and put it in the column 'Rs [kN]'\n",
    "validation_data_CB[\"Rs [kN]\"] = \\\n",
    "    (np.pi * validation_data_CB[\"Diameter [m]\"] * \\\n",
    "     validation_data_CB[\"z [m]\"].diff() * validation_data_CB[\"qc [MPa]\"]).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate linearized ENTHRU, Rs and z\n",
    "validation_data_CB[\"linearized ENTHRU\"] = np.tanh(5 * validation_data_CB[\"Normalised ENTRHU [-]\"] - 0.5)\n",
    "validation_data_CB[\"linearized Rs\"] = np.tanh(0.001 * validation_data_CB[\"Rs [kN]\"] - 1)\n",
    "validation_data_CB[\"linearized z\"] = np.tanh(0.1 * validation_data_CB[\"z [m]\"] - 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the matrix with n samples and 3 features\n",
    "X_validation = validation_data_CB.dropna()[linearized_features]\n",
    "# Create the vector with n observations of blowcount\n",
    "y_validation = validation_data_CB.dropna()[\"Blowcount [Blows/m]\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our fitted model, we can now calculate the $ R^2 $ score for our validation data. The score is relatively high and we can conclude that the model generalises well. If this validation score would be low, we would have to re-evaluate our feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the R2 score for the validation data\n",
    "model_4.score(X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the predicted blowcounts for our validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_predictions = model_4.predict(X_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions (red dots) can be plotted against the actual observed blowcounts. The cone resistance and normalised ENTRHU are also plotted for information.\n",
    "\n",
    "The predictions are reasonable and follow the general trend fairly well. In the layer with lower cone resistance below (10-15m depth), there is an overprediction of blowcount. This is due to the relatively limited amount of datapoints with low cone resistance in the training data. Further model refinement could address this issue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = subplots.make_subplots(rows=1, cols=3, print_grid=False, shared_yaxes=True)\n",
    "trace1 = go.Scatter(\n",
    "    x=validation_data_CB[\"qc [MPa]\"],\n",
    "    y=validation_data_CB[\"z [m]\"], showlegend=False, mode='lines',name='qc')\n",
    "fig.append_trace(trace1, 1, 1)\n",
    "trace2 = go.Scatter(\n",
    "    x=validation_data_CB[\"Normalised ENTRHU [-]\"],\n",
    "    y=validation_data_CB[\"z [m]\"], showlegend=False, mode='lines',name='ENTRHU')\n",
    "fig.append_trace(trace2, 1, 2)\n",
    "trace3 = go.Scatter(\n",
    "    x=validation_data_CB[\"Blowcount [Blows/m]\"],\n",
    "    y=validation_data_CB[\"z [m]\"], showlegend=False, mode='lines',name='Blowcount')\n",
    "fig.append_trace(trace3, 1, 3)\n",
    "\n",
    "trace_predictions = go.Scatter(\n",
    "    x=validation_predictions,\n",
    "    y=validation_data_CB.dropna()[\"z [m]\"], showlegend=False, mode='markers',name='Blowcount predictions')\n",
    "fig.append_trace(trace_predictions, 1, 3)\n",
    "\n",
    "fig['layout']['xaxis1'].update(title=r'$ q_c \\ \\text{[MPa]} $', side='top', anchor='y', range=(0, 100), dtick=10)\n",
    "fig['layout']['xaxis2'].update(title=r'$ \\text{Normalised ENTRHU [-]} $', side='top', anchor='y',\n",
    "                               range=(0, 1), dtick=0.2)\n",
    "fig['layout']['xaxis3'].update(title=r'$ \\text{Observed blowcount [Blows/m]} $', side='top', anchor='y',\n",
    "                               range=(0, 200), dtick=25)\n",
    "fig['layout']['yaxis1'].update(title=r'$ z \\ \\text{[m]} $', range=(40, 0), dtick=2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of validation can be automated. The [scikit-learn documentation](https://scikit-learn.org/stable/modules/cross_validation.html) has further details on this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This tutorial shows how a basic machine learning model can be built up. The workflow shows the importance of integrating engineering knowledge and to ensure that the models make physical sense.\n",
    "\n",
    "A machine learning workflow is not fundamentally different from a conventional workflow for fitting a semi-empirical model. The methods available in scikit-learn make the process scaleable to large datasets with only a few lines of code.\n",
    "\n",
    "The workflow will always consist of the following steps:\n",
    "\n",
    "   - Select features for the machine learning. Use engineering knowledge to construct features which have a better correlation with the target variable under consideration;\n",
    "   - Split the dataset in a training dataset and a validation dataset;\n",
    "   - Select the type of machine learning model you want to use (e.g. Linear regression, Support Vector Machines, Neural Nets, ...);\n",
    "   - Train the model using the training data;\n",
    "   - Validate the model on the validation data;\n",
    "\n",
    "If the validation is successful, the model can be used for predictions on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice\n",
    "\n",
    "We will continue with regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "The internet has a large amount of information available on machine learning. Here are a couple of suggestions to guide your reading:\n",
    "\n",
    "   - [scikit-learn documentation](https://scikit-learn.org/stable/tutorial/basic/tutorial.html): The scikit-learn package has extensive documentation and provides good general-purpose tutorials;\n",
    "   - [10 minutes to Pandas](http://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html): The Pandas package is used extensively in this tutorial to facilitate the data processing. Getting to grips with Pandas is a good idea for every aspiring data enthousiast. The Pandas documentation is extensive and this guide provides you with a good overview of the capabilities of the package;\n",
    "   - [Towards data science](https://towardsdatascience.com/): A blog with frequent posts on data science topics with contributions for the novice and advanced data scientist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
